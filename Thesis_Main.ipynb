{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using pip 25.2 from c:\\Users\\Rakibul Hassan\\anaconda3\\envs\\ml\\Lib\\site-packages\\pip (python 3.11)\n",
            "Requirement already satisfied: transformers in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (4.57.1)\n",
            "Requirement already satisfied: torch in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: librosa in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (0.11.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: opencv-python-headless in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (3.10.7)\n",
            "Requirement already satisfied: filelock in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (0.62.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (5.2.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: colorama in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rakibul hassan\\anaconda3\\envs\\ml\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers torch torchvision torchaudio librosa scikit-learn opencv-python-headless matplotlib -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio transformers librosa scikit-learn matplotlib pandas seaborn opencv-python -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rakibul Hassan\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor, BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "✅ GPU detected: NVIDIA GeForce RTX 2080 SUPER\n"
          ]
        }
      ],
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"❌ Still running on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 1. Preprocessing\n",
        "# ========================\n",
        "\n",
        "def preprocess_audio(file_path, sr=16000):\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "    y = librosa.effects.trim(y)[0]\n",
        "    y = librosa.util.normalize(y)\n",
        "    return y\n",
        "\n",
        "def preprocess_video(video_path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    frames = np.array(frames).astype(np.float32) / 255.0\n",
        "    return frames\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text.lower().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================\n",
        "# 2. Feature Extraction Models\n",
        "# ========================\n",
        "\n",
        "# ensure device is defined before moving models to it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Audio embeddings: Wav2Vec2\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
        "\n",
        "def extract_audio_features(y):\n",
        "    inputs = processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k,v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = wav2vec(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "# Text embeddings: BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "def extract_text_features(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k,v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(**inputs)\n",
        "    return outputs.pooler_output.cpu().numpy()\n",
        "\n",
        "# Video embeddings: ResNet50\n",
        "resnet = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1]).to(device)  # remove classifier\n",
        "resnet.eval()\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
        "\n",
        "def extract_video_features(frames):\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            img = transform(frame).unsqueeze(0).to(device)\n",
        "            feat = resnet(img)\n",
        "            embeddings.append(feat.cpu().numpy().squeeze())\n",
        "    return np.mean(embeddings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 3. Dataset & Dataloader\n",
        "# ========================\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = samples\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.samples[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total .wav found: 5856\n",
            "Audio: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\wav\\Ses01F_impro01.wav\n",
            "Video: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro01.avi\n",
            "Text : F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\EmoEvaluation\\Ses01F_impro01.txt\n",
            "Audio: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\wav\\Ses01F_impro02.wav\n",
            "Video: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro02.avi\n",
            "Text : F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\EmoEvaluation\\Ses01F_impro02.txt\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro02.avi | Using zeros (Unable to allocate 3.47 GiB for an array with shape (6185, 224, 224, 3) and data type float32)\n",
            "Audio: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\wav\\Ses01F_impro03.wav\n",
            "Video: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro03.avi\n",
            "Text : F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\EmoEvaluation\\Ses01F_impro03.txt\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro03.avi | Using zeros (Unable to allocate 2.07 GiB for an array with shape (3694, 224, 224, 3) and data type float32)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro04.avi | Using zeros (Unable to allocate 864. MiB for an array with shape (6019, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro05.avi | Using zeros (Unable to allocate 3.62 GiB for an array with shape (6451, 224, 224, 3) and data type float32)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro06.avi | Using zeros (OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1036800 bytes in function 'cv::OutOfMemoryError'\n",
            ")\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_impro07.avi | Using zeros (Unable to allocate 614. MiB for an array with shape (4274, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script01_1.avi | Using zeros (Unable to allocate 1.80 GiB for an array with shape (12866, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script01_2.avi | Using zeros (Unable to allocate 2.55 GiB for an array with shape (4542, 224, 224, 3) and data type float32)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script01_3.avi | Using zeros (OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1036800 bytes in function 'cv::OutOfMemoryError'\n",
            ")\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script02_1.avi | Using zeros (OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1036800 bytes in function 'cv::OutOfMemoryError'\n",
            ")\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script02_2.avi | Using zeros (OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1036800 bytes in function 'cv::OutOfMemoryError'\n",
            ")\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script03_1.avi | Using zeros (<built-in function cvtColor> returned a result with an exception set)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01F_script03_2.avi | Using zeros (<built-in function cvtColor> returned a result with an exception set)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro01.avi | Using zeros (Unable to allocate 724. MiB for an array with shape (5046, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro02.avi | Using zeros (Unable to allocate 942. MiB for an array with shape (6563, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro03.avi | Using zeros (Unable to allocate 676. MiB for an array with shape (4709, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro04.avi | Using zeros (Unable to allocate 818. MiB for an array with shape (5698, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro05.avi | Using zeros (Unable to allocate 883. MiB for an array with shape (6148, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro06.avi | Using zeros (<built-in function cvtColor> returned a result with an exception set)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_impro07.avi | Using zeros (Unable to allocate 3.52 GiB for an array with shape (6280, 224, 224, 3) and data type float32)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_script01_1.avi | Using zeros (Unable to allocate 1.80 GiB for an array with shape (12811, 224, 224, 3) and data type uint8)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_script01_2.avi | Using zeros (Unable to allocate 2.82 GiB for an array with shape (5027, 224, 224, 3) and data type float32)\n",
            "[Warning] Video failed: F:\\Reserach\\Reserach\\IEMOCAP_full_release\\Session1\\dialog\\avi\\DivX\\Ses01M_script01_3.avi | Using zeros (<built-in function cvtColor> returned a result with an exception set)\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# 4. Load Data & Extract Features (robust file discovery)\n",
        "# ========================\n",
        "\n",
        "DATASET_PATH = Path(r\"F:\\Reserach\\Reserach\\IEMOCAP_full_release\")\n",
        "N = 50\n",
        "samples = []\n",
        "\n",
        "wav_files = list(DATASET_PATH.glob(\"**/*.wav\"))\n",
        "print(\"Total .wav found:\", len(wav_files))\n",
        "wav_files = wav_files[:N]\n",
        "\n",
        "video_exts = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
        "text_exts = [\".txt\", \".script\", \".trs\", \".annot\"]\n",
        "\n",
        "def find_matching_file(stem, exts):\n",
        "    # search for same-stem file with any of the exts anywhere under dataset\n",
        "    for ext in exts:\n",
        "        candidate = DATASET_PATH.joinpath(f\"**/{stem}{ext}\")\n",
        "        matches = list(DATASET_PATH.glob(f\"**/{stem}{ext}\"))\n",
        "        if matches:\n",
        "            return matches[0]\n",
        "    # fallback: return None\n",
        "    return None\n",
        "\n",
        "for p in wav_files:\n",
        "    audio_file = str(p)\n",
        "    stem = p.stem\n",
        "    # find video candidate (same stem)\n",
        "    video_path = None\n",
        "    for ext in video_exts:\n",
        "        v = p.with_suffix(ext)\n",
        "        if v.exists():\n",
        "            video_path = v\n",
        "            break\n",
        "    if video_path is None:\n",
        "        video_path = find_matching_file(stem, video_exts)\n",
        "    # find text candidate (same stem)\n",
        "    text_path = None\n",
        "    for ext in text_exts:\n",
        "        t = p.with_suffix(ext)\n",
        "        if t.exists():\n",
        "            text_path = t\n",
        "            break\n",
        "    if text_path is None:\n",
        "        text_path = find_matching_file(stem, text_exts)\n",
        "\n",
        "    if video_path is None:\n",
        "        video_file = None\n",
        "    else:\n",
        "        video_file = str(video_path)\n",
        "\n",
        "    if text_path is None:\n",
        "        text_file = None\n",
        "    else:\n",
        "        text_file = str(text_path)\n",
        "\n",
        "    # quick debug print for first few\n",
        "    if len(samples) < 3:\n",
        "        print(\"Audio:\", audio_file)\n",
        "        print(\"Video:\", video_file)\n",
        "        print(\"Text :\", text_file)\n",
        "\n",
        "    label = \"happy\"  # placeholder — replace with real labels if available\n",
        "\n",
        "    # ----------------------\n",
        "    # Audio Feature\n",
        "    # ----------------------\n",
        "    try:\n",
        "        audio_feat = extract_audio_features(preprocess_audio(audio_file))\n",
        "    except Exception as e:\n",
        "        print(f\"[Warning] Audio failed: {audio_file} | Using zeros ({e})\")\n",
        "        audio_feat = np.zeros((1, 768))\n",
        "\n",
        "    # ----------------------\n",
        "    # Video Feature\n",
        "    # ----------------------\n",
        "    if video_file and os.path.exists(video_file):\n",
        "        try:\n",
        "            video_feat = extract_video_features(preprocess_video(video_file))\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Video failed: {video_file} | Using zeros ({e})\")\n",
        "            video_feat = np.zeros(2048)\n",
        "    else:\n",
        "        video_feat = np.zeros(2048)\n",
        "    video_feat = video_feat.reshape(1, -1)\n",
        "\n",
        "    # ----------------------\n",
        "    # Text Feature\n",
        "    # ----------------------\n",
        "    if text_file and os.path.exists(text_file):\n",
        "        try:\n",
        "            with open(text_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Text read failed: {text_file} | Using dummy text ({e})\")\n",
        "            text = \"dummy text\"\n",
        "    else:\n",
        "        text = \"dummy text\"\n",
        "\n",
        "    try:\n",
        "        text_feat = extract_text_features(preprocess_text(text))\n",
        "    except Exception as e:\n",
        "        print(f\"[Warning] Text feature extraction failed for {text_file} | Using zeros ({e})\")\n",
        "        text_feat = np.zeros((1, 768))\n",
        "\n",
        "    # ----------------------\n",
        "    # Combine features\n",
        "    # ----------------------\n",
        "    try:\n",
        "        combined = np.concatenate([audio_feat, video_feat, text_feat], axis=1).squeeze()\n",
        "        samples.append((combined, label))\n",
        "    except Exception as e:\n",
        "        print(f\"[Warning] Feature concat failed for {audio_file} | Skipping ({e})\")\n",
        "        continue\n",
        "\n",
        "print(f\"Total samples loaded: {len(samples)}\")\n",
        "\n",
        "# ----------------------\n",
        "# Prepare dataset (same as before)\n",
        "# ----------------------\n",
        "X = np.array([s[0] for s in samples])\n",
        "y = np.array([s[1] for s in samples])\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "train_dataset = MultiModalDataset(X_train, y_train)\n",
        "test_dataset = MultiModalDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 5. LSTM + Attention Model\n",
        "# ========================\n",
        "\n",
        "class LSTMAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        weights = torch.softmax(self.attn(x), dim=1)\n",
        "        x = torch.sum(weights * x, dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(y_train))\n",
        "\n",
        "model = LSTMAttention(input_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.0000\n",
            "Epoch 2, Loss: 0.0000\n",
            "Epoch 3, Loss: 0.0000\n",
            "Epoch 4, Loss: 0.0000\n",
            "Epoch 5, Loss: 0.0000\n",
            "Epoch 6, Loss: 0.0000\n",
            "Epoch 7, Loss: 0.0000\n",
            "Epoch 8, Loss: 0.0000\n",
            "Epoch 9, Loss: 0.0000\n",
            "Epoch 10, Loss: 0.0000\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ========================\n",
        "# 6. Training\n",
        "# ========================\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.unsqueeze(1).to(device)  # LSTM expects (batch, seq, feature), seq=1\n",
        "        yb = yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    train_losses.append(epoch_loss/len(train_loader))\n",
        "    print(f\"Epoch {epoch+1}, Loss: {train_losses[-1]:.4f}\")\n",
        "\n",
        "# Save loss plot\n",
        "plt.figure()\n",
        "plt.plot(train_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"train_loss.png\")\n",
        "\n",
        "# Save LSTM model\n",
        "torch.save(model.state_dict(), \"lstm_attention.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 7. RandomForest + Ensemble\n",
        "# ========================\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# LSTM predictions\n",
        "model.eval()\n",
        "y_lstm = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.unsqueeze(1).to(device)\n",
        "        out = model(xb)\n",
        "        y_lstm.extend(torch.softmax(out, dim=1).cpu().numpy())\n",
        "y_lstm = np.array(y_lstm)\n",
        "\n",
        "# RF predictions\n",
        "y_rf = rf.predict_proba(X_test)\n",
        "\n",
        "# Ensemble\n",
        "final_pred = np.argmax((y_lstm + y_rf)/2, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 8. Evaluation\n",
        "# ========================\n",
        "\n",
        "acc = accuracy_score(y_test, final_pred)\n",
        "prec = precision_score(y_test, final_pred, average=\"weighted\")\n",
        "rec = recall_score(y_test, final_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test, final_pred, average=\"weighted\")\n",
        "roc = roc_auc_score(y_test, rf.predict_proba(X_test), multi_class=\"ovr\")\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"Precision: {prec:.3f}\")\n",
        "print(f\"Recall: {rec:.3f}\")\n",
        "print(f\"F1: {f1:.3f}\")\n",
        "print(f\"ROC AUC: {roc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
